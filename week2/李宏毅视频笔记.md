- ## 李宏毅视频笔记第二周

- #### 机器学习核心框架：找函数的 “三步法”

- 机器学习的本质是从数据中找到一个能解决特定问题的最优函数，整个过程遵循 “模型定义 - 损失衡量 - 优化求解” 的三步框架

- - **模型（Model）**：定义候选函数的集合。

- - **损失函数（Loss Function）**：衡量函数的好坏。通过计算预测值与真实值的差异（如分类任务的交叉熵、回归任务的均方误差），损失越小说明函数越优。

- - **优化器（Optimizer）**：寻找最优函数。基于损失函数的梯度信息，通过梯度下降（SGD、Adam 等）调整模型参数，最小化损失，最终得到性能最佳的函数。

- #### 监督学习（Supervised Learning）

- - **核心原理**：利用带有 “输入 - 输出” 标签的数据训练模型，学习输入到输出的映射关系。

- - **关键特点**：依赖高质量标注数据，适用于有明确标准答案的场景。

- #### 无监督学习（Unsupervised Learning）

- - **核心原理**：仅利用无标签数据，挖掘数据内在的结构或模式。

- - **关键特点**：不依赖标注，适用于探索数据潜在规律，但结果解释性可能较弱。

- #### 强化学习（Reinforcement Learning）

- - **核心原理**：智能体通过与环境交互，基于奖励信号学习最优行动策略，目标是最大化累积奖励。

- - **关键机制**：

- - - 策略（Policy）：智能体选择行动的规则（如游戏 AI 的 “向左 / 向右移动” 决策）。

- - - 奖励（Reward）：环境对行动的反馈（如游戏得分、任务完成信号）。

- - - 价值函数（Value Function）：评估状态或行动的长期收益，指导策略优化。

- #### 神经网络的基本结构

- - **神经元（Neuron）**：模拟生物神经元，接收输入后通过线性变换（权重 × 输入 + 偏置）和非线性激活（ReLU、sigmoid 等）输出结果，激活函数引入非线性，使模型能拟合复杂关系。

- - **层级结构**：由输入层（接收数据）、隐藏层（特征提取）、输出层（输出结果）组成，隐藏层越多（深度学习），特征提取能力越强，可学习更抽象的模式（如从图像像素→边缘→物体部件→完整物体）。

- #### 训练原理：反向传播（Backpropagation）

- - **前向传播**：输入数据通过网络计算，得到预测值并计算损失。

- - **反向传播**：从输出层开始，通过链式法则计算损失对各层参数（权重、偏置）的梯度，再用梯度下降更新参数，逐步降低损失。

- #### 卷积神经网络（CNN）

- - **核心原理**：通过卷积层的滑动窗口（卷积核）提取局部特征（如图像的边缘、纹理），池化层减少特征维度，全连接层整合特征输出结果。

- #### 循环神经网络（RNN）与 Transformer

- - **RNN**：通过循环结构处理序列数据，能捕捉前后依赖关系，但存在长距离依赖建模困难的问题。

- - **Transformer**：基于自注意力（Self-Attention）机制，可并行处理序列中所有位置，同时关注不同位置的关联（如长句子中远距离词汇的语义联系），是 ChatGPT 等大语言模型的核心架构，适用于自然语言处理（翻译、生成）等任务。

- #### 模型泛化与过拟合问题

- - **过拟合（Overfitting）**：模型在训练数据上表现极好，但在新数据上性能骤降，原因是过度学习训练数据中的噪声而非通用规律。

- - **解决方法**：

- - - 正则化（L1/L2 正则、Dropout）：限制模型复杂度，防止参数过度拟合噪声。

- - - 数据增强：扩充训练数据（如对图像旋转、裁剪），提高模型对数据变化的适应力。

- - - 早停（Early Stopping）：在验证集损失不再下降时停止训练，避免过拟合。

- - **泛化能力**：模型对新数据的适应能力，是衡量模型实用性的核心指标，需通过训练集、验证集、测试集的合理划分评估。