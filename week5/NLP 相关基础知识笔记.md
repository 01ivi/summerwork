# NLP 相关基础知识笔记

## 一、自然语言处理导引及词向量

自然语言处理（NLP）是人工智能的重要分支，旨在让计算机理解、处理和生成人类语言。词向量（Word Vector）是 NLP 中表示词语的核心技术，解决了传统离散表示（如 One-hot）无法捕捉语义关联的问题。

### 1. 词向量的核心意义

- 传统词语表示（如 One-hot）的缺陷：维度灾难（词汇表大小即维度）、无法体现词语语义关联（如 “国王” 与 “女王” 的关联性）。
- 词向量的本质：将词语映射到低维连续向量空间，向量的距离 / 相似度对应词语的语义关联（如 “北京” 与 “中国” 的向量关系近似 “华盛顿” 与 “美国”）。

### 2. Word2vec 详解

Word2vec 是 2013 年由 Mikolov 等人提出的词向量训练模型，通过优化 “预测上下文” 或 “根据上下文预测目标词” 的任务，高效学习词向量。

#### （1）模型结构：两种核心架构

- **CBOW（Continuous Bag-of-Words）**：根据上下文词语（如 “[我] 吃 [饭]” 中的 “我” 和 “饭”）预测目标词（“吃”）。
  输入：上下文词向量的平均 / 总和；输出：目标词的概率分布。
- **Skip-gram**：根据目标词（如 “吃”）预测上下文词语（“我” 和 “饭”）。
  相比 CBOW，更适合处理低频词，对语义的捕捉更精准。

#### （2）训练优化：提升效率的关键技术

- **Hierarchical Softmax**（来自论文 1《Efficient Estimation of Word Representations in Vector Space》）：
  用二叉树（ Huffman 树）替代传统 Softmax 的全量词汇表计算，将词表大小为 V 的复杂度从 O (V) 降为 O (logV)，大幅提升训练速度。
- **Negative Sampling**（来自论文 2《Distributed Representations of Words and Phrases and their Compositionality》）：
  不再计算所有负例的概率，而是随机采样少量负例（如 5-20 个），通过 “区分目标词与负例” 优化目标，进一步提升效率，且效果更优。

#### （3）参数学习原理

- Word2vec 的参数包括 “输入词向量矩阵”（目标词的向量）和 “输出权重矩阵”（上下文预测的参数）。
- 学习过程本质是通过反向传播优化 “预测准确率”，最终保留输入词向量矩阵作为词向量（输出矩阵通常丢弃）。
- 关键发现：Skip-gram+Negative Sampling 在语义和语法任务（如词性类比）上表现最佳。

### 3. 词向量的延伸

- 短语向量：论文 2 中提出将频繁共现的短语（如 “New York”）作为单个 “词” 处理，提升表达能力。
- 应用：作为下游任务（如分类、翻译）的输入特征，显著提升模型性能。

## 二、依赖分析

依赖分析是句法分析的核心任务，旨在揭示句子中词语之间的 “依赖关系”（如 “主谓”“动宾”），体现词语的语法角色和逻辑关联。

### 1. 依赖关系的定义

- 本质：句子中词语间的有向边，如 “小明吃苹果” 中，“吃” 是核心词，“小明”（主谓）和 “苹果”（动宾）依赖于 “吃”。
- 特点：通常是一棵以 “根节点”（无父节点的核心词）为中心的树结构，每个词（除根节点外）有且仅有一个父节点。

### 2. 依赖分析的类型

- **Projective Dependency**：依赖边不交叉（适合多数语言，如中文、英文）。
- **Non-projective Dependency**：依赖边可交叉（适合屈折语，如德语）。

### 3. 主流算法

- **基于转移的方法（Transition-based Parsing）**：
  通过一系列动作（如 “移进”“归约”“左弧”“右弧”）构建依赖树，代表算法为 Arc-Standard。
  优势：效率高（线性时间复杂度），适合实时场景。
- **基于图的方法（Graph-based Parsing）**：
  将依赖分析转化为 “寻找最大生成树” 问题，通过打分函数（如神经网络）计算边的权重，选择总权重最大的树。
  优势：全局最优性，适合高精度场景。

### 4. 评价指标

- **UAS（Unlabeled Attachment Score）**：正确预测依赖边（不考虑关系类型）的比例。
- **LAS（Labeled Attachment Score）**：正确预测依赖边及关系类型的比例（更严格）。

## 三、语言模型

语言模型（LM）是 NLP 的基础组件，用于计算 “句子是人类语言的概率”，核心是建模词语序列的联合概率分布。

### 1. 语言模型的定义

- 对于句子*S*=*w*1,*w*2,...,*w**n*，语言模型计算*P*(*S*)=*P*(*w*1)*P*(*w*2∣*w*1)*P*(*w*3∣*w*1,*w*2)...*P*(*w**n*∣*w*1,...,*w**n*−1)。
- 作用：用于语音识别（选择最可能的文本）、机器翻译（评估译文流畅度）、文本生成（生成合理序列）等。

### 2. 传统语言模型：n-gram 模型

- 核心假设：一个词的出现仅依赖于前*n*−1个词（如 bigram 依赖前 1 个词，trigram 依赖前 2 个词）。
- 缺陷：
  - 数据稀疏性（长序列可能从未出现，导致概率为 0）；
  - 无法捕捉长距离依赖（如 “他说… 他很高兴” 中两个 “他” 的关联）。
- 平滑技术（解决稀疏性）：
  - Add-one（加 1 平滑）：对所有 n-gram 计数 + 1；
  - Kneser-Ney：考虑 “词的预测能力”，对低频 n-gram 更友好。

### 3. 神经网络语言模型（NNLM）

- 用神经网络建模条件概率*P*(*w**t*∣*w**t*−*n*+1,...,*w**t*−1)，解决 n-gram 的缺陷。

- RNN 语言模型

  ：

  - 用循环神经网络（RNN）处理序列，通过隐藏状态保存历史信息，理论上可捕捉任意长距离依赖。
  - 缺陷：梯度消失 / 爆炸（难以学习长期依赖）。

- 改进变体

  ：

  - LSTM（长短期记忆网络）：通过 “门控机制”（输入门、遗忘门、输出门）控制信息流动，缓解梯度问题。
  - GRU（门控循环单元）：简化 LSTM 结构，保留核心能力，训练更快。

### 4. 评价指标：困惑度（Perplexity, PPL）

- 定义：*PP**L*(*S*)=*P*(*S*)−1/*n*，表示模型对句子的 “困惑程度”。
- 意义：PPL 越低，模型对语言的建模能力越强（如人类语言的 PPL 接近 1）。

## 四、Transformer 模型及预训练模型

Transformer 是 2017 年提出的革命性模型，基于自注意力机制，彻底改变了 NLP 的范式；预训练模型则通过 “预训练 - 微调” 模式，大幅提升下游任务性能。

### 1. Transformer 的核心结构

- **整体框架**：编码器（Encoder）+ 解码器（Decoder），均由 N 个 “子层” 堆叠而成。
- **自注意力机制（Self-Attention）**：
  让每个词 “关注” 序列中其他相关词，计算方式：
  *A**tt**e**n**t**i**o**n*(*Q*,*K*,*V*)=*so**f**t**ma**x*(*d**k*​​*Q**K**T*​)*V*
  其中 Q（查询）、K（键）、V（值）由输入通过线性变换得到，*d**k*​为维度（避免梯度不稳定）。
- **多头注意力（Multi-Head Attention）**：
  将 Q/K/V 拆分为 h 组，分别计算注意力后拼接，捕捉不同维度的关联（如语义、语法）。
- **位置编码（Positional Encoding）**：
  由于 Transformer 无循环结构，需通过正弦 / 余弦函数或可学习参数注入位置信息，确保模型理解词序。
- **优势**：并行计算（优于 RNN 的串行）、长距离依赖建模（优于 RNN 的短期记忆）。

### 2. 预训练模型的范式

- 核心思想：在大规模无标注文本上预训练模型，学习通用语言知识，再针对下游任务微调。

#### （1）代表性模型

- **ELMo（Embeddings from Language Models）**：
  用双向 LSTM 训练两个单向语言模型（左到右 + 右到左），拼接输出作为上下文相关词向量（解决 Word2vec 的静态缺陷）。

- BERT（Bidirectional Encoder Representations from Transformers）

  ：

  基于 Transformer 编码器，通过两个任务预训练：

  - Masked LM（随机遮盖 15% 的词，预测被遮盖词）；
  - Next Sentence Prediction（预测两个句子是否连续）。
    优势：真正的双向建模，在分类、问答等任务上大幅刷新 SOTA。

- **GPT 系列（Generative Pre-trained Transformer）**：
  基于 Transformer 解码器，采用自回归语言模型（从左到右预测下一个词）预训练，擅长生成任务（如文本续写、对话）。

## 五、大模型相关

大模型（Large Language Model, LLM）是参数量达百亿 / 千亿级的预训练模型，具备 “涌现能力”（小模型不具备的复杂能力）。

### 1. 大模型基础

- **核心特征**：参数量大（如 GPT-3 有 1750 亿参数）、训练数据量大（万亿级 token）、依赖算力（数千 GPU 集群）。
- **涌现能力**：随参数量增长，突然具备的能力（如逻辑推理、多语言翻译、零样本学习）。

### 2. 清华大模型课程核心内容（参考方向）

- 训练流程：数据清洗（去重、过滤有害内容）→ 预训练（优化语言模型目标）→ 对齐（使模型符合人类偏好）。
- 关键技术：
  - 混合专家模型（MoE）：将模型拆分为多个 “专家”，每次输入激活部分专家，提升参数量同时控制计算成本；
  - 指令微调（Instruction Tuning）：用 “指令 - 输出” 数据微调，提升模型对任务的理解能力；
  - RLHF（基于人类反馈的强化学习）：通过人类偏好数据训练奖励模型，再用强化学习优化模型输出。
- 伦理与安全：偏见缓解、虚假信息生成、隐私保护。

### 3. 大模型关键知识点

- 多模态融合：大模型从纯文本扩展到图文、语音等多模态（如 GPT-4V、LLaVA）。
- 高效部署：模型压缩（量化、剪枝）、推理加速（如 Flash Attention）。

## 六、多模态学习

多模态学习旨在融合文本、图像、语音等不同模态的信息，解决跨模态理解与生成问题。

### 1. 核心挑战

- 模态差异：文本是离散符号，图像是连续像素，结构和特征空间不同；
- 数据对齐：跨模态数据的语义关联（如 “猫” 的文本与猫的图像）需精准匹配。

### 2. 经典论文与方法

- **CLIP（Contrastive Language-Image Pretraining）**：
  用对比学习训练文本编码器和图像编码器，使 “匹配的图文对” 向量相似，“不匹配的” 向量疏远。
  优势：零样本迁移能力（可直接用于未见过的分类任务）。
- **ALBEF（Aligning Language and Vision with BERT）**：
  引入跨模态注意力机制，让文本和图像互相 “关注” 相关区域（如文本 “红色” 关注图像中的红色物体），提升细粒度对齐。
- **DALL·E / Stable Diffusion**：
  文本生成图像的代表模型，通过扩散模型或自回归模型，将文本语义转化为图像像素分布。