# 一、Text-to-Text Transfer Transformer

## 1.1 核心设计理念：“一切皆文本转换”

- 统一任务框架

  ：T5 将所有自然语言处理（NLP）任务统一为 “文本到文本（Text-to-Text）” 形式，即输入和输出均为文本。

  - 例：
    - 翻译任务：输入 “translate English to German: Hello world”，输出 “Hallöchen Welt”；
    - 情感分析：输入 “sst2 sentence: This movie is great”，输出 “positive”；
    - 摘要任务：输入 “summarize: ...（长文本）”，输出 “...（摘要）”。

- **优势**：简化模型设计（无需为不同任务定制输出头）、统一训练目标（均为预测输出文本）、提升跨任务迁移能力。

## 1.2 模型结构与预训练

- **基础架构**：基于 Transformer 的 Encoder-Decoder 结构（与 BERT 的仅 Encoder、GPT 的仅 Decoder 不同），支持双向上下文理解（Encoder）和生成式输出（Decoder）。

- 预训练任务

  ：

  - Span Corruption

    ：随机掩盖输入文本中的连续片段（“span”），用特殊符号 “<extra_id_i>” 标记掩盖位置，要求模型预测被掩盖的内容并按顺序填充。

    - 例：输入 “Alice <extra_id_0> to the <extra_id_1>”，目标输出 “<extra_id_0> went <extra_id_1> park”。

  - 优势：相比 BERT 的单 token 掩码，更接近真实场景中的文本补全需求，提升生成能力。

- **预训练数据**：C4（Colossal Clean Crawled Corpus），包含 15TB 公开网页文本，经过去重、过滤低质量内容处理。

## 1.3 模型规模与性能

- **规模变体**：T5 提供多种参数规模（Small: 60M、Base: 220M、Large: 770M、3B、11B），更大规模模型在多数任务上表现更优。
- **性能表现**：在 GLUE、SQuAD 等经典 NLP benchmark 上刷新当时 SOTA（State-of-the-Art），尤其在生成式任务（如摘要、翻译）中优势显著。

## 1.4 关键启示

- 任务统一化是大模型泛化能力的重要设计思路，为后续 LLM 的 “通用任务处理” 奠定基础；
- 预训练任务的设计需贴近下游需求（如 Span Corruption 更适合生成任务），数据质量与规模同样关键。

# 二、Large Language Model

## 2.1 定义与核心特点

- **定义**：指参数规模达数十亿至数万亿、基于 Transformer 架构、通过海量文本预训练的语言模型，具备强大的上下文理解与生成能力。

- 核心特点

  ：

  - **规模效应**：参数规模（从 GPT-3 的 175B 到 GPT-4 的万亿级）与性能正相关，当规模超过阈值后会涌现出新能力（如逻辑推理、少样本学习）；
  - **通用能力**：无需针对特定任务修改模型结构，通过提示词（Prompt）即可完成翻译、写作、代码生成等多样任务；
  - **自回归生成**：多数 LLM 基于 Transformer 解码器，采用 “自回归” 方式生成文本（每次预测下一个 token，依赖前文）。

## 2.2 发展历程与典型模型

| 模型     | 发布方    | 参数规模       | 核心特点                                                     |
| -------- | --------- | -------------- | ------------------------------------------------------------ |
| GPT-3    | OpenAI    | 175B           | 首次验证 “规模即能力”，支持少样本学习（Few-shot Learning）   |
| PaLM     | Google    | 540B           | 采用 “路径并行” 优化训练效率，在多语言任务和推理任务上表现突出 |
| LLaMA 2  | Meta      | 7B-70B         | 开源可商用，训练数据包含对话数据，适合微调为对话模型         |
| Claude 2 | Anthropic | 未知（超百亿） | 支持更长上下文（100k tokens），安全性与可解释性优化          |
| GPT-4    | OpenAI    | 万亿级         | 多模态能力（文本 + 图像），逻辑推理与复杂任务处理能力显著提升 |

## 2.3 训练流程：从预训练到应用

1. **预训练（Pre-training）**：
   - 目标：学习通用语言规律（语法、语义、世界知识）。
   - 数据：海量文本（书籍、网页、代码等，如 GPT-3 训练数据约 45TB）。
   - 任务：自回归语言建模（预测下一个 token 的概率，如 “我爱____” 中预测 “中国”）。
2. **微调（Fine-tuning）**：
   - 目标：适配特定场景（如对话、客服）。
   - 数据：领域内高质量数据（如 ChatGPT 的 SFT 阶段使用人工标注的对话数据）。
   - 方式：冻结部分预训练参数，用小学习率训练，保留通用能力的同时优化特定任务表现。
3. **对齐（Alignment）**：
   - 目标：让模型输出符合人类价值观（安全、无害、有用）。
   - 核心技术：RLHF（基于人类反馈的强化学习），通过人类对模型输出的排序训练奖励模型（RM），再用强化学习（如 PPO）优化模型。

## 2.4 挑战与局限

- **计算成本**：训练万亿级参数模型需数千 GPU/TPU 运行数周，能耗与成本极高；
- **偏见与安全**：训练数据中的偏见（如性别、种族）会被模型学习，可能生成有害内容；
- **可解释性**：“黑箱” 特性导致难以解释模型决策逻辑（如为何生成某一回答）；
- **幻觉（Hallucination）**：可能生成看似合理但与事实不符的内容（如虚构学术引用）。

# 三、生成式 AI ChatGPT 原理

## P1-P3：ChatGPT 的基础架构与核心原理

### 核心定位

ChatGPT 是基于 GPT 架构（Transformer 解码器）的对话模型，核心目标是通过自然语言交互完成用户任务，其能力源于 “预训练 + 微调 + 对齐” 的三阶流程。

### 技术基底：GPT 架构

- **仅用 Transformer 解码器**：放弃 Encoder，仅保留 Decoder 的 “自注意力机制”，支持长上下文依赖（如理解多轮对话历史）；
- **自回归生成**：生成文本时从左到右逐 token 预测，每个 token 依赖前文所有信息（如对话中 “它” 的指代需结合上文确定）。

### 与传统对话系统的区别

- 传统系统依赖规则或小模型，只能处理固定场景（如客服 FAQ）；
- ChatGPT 依赖大模型的通用能力，无需预设规则即可应对开放域对话（如 “写一首关于春天的诗”“解释相对论”）。

## P7-P14：ChatGPT 的训练细节与能力边界

### 1. SFT（监督微调）：让模型 “会对话”

- **数据来源**：人工标注的高质量对话（人类扮演 “用户” 和 “助手” 生成的多轮对话），确保多样性（日常聊天、知识问答、任务指令）；
- **训练目标**：让模型学习 “用户输入→合理回应” 的映射，例如用户问 “推荐一部科幻电影”，模型输出 “《星际穿越》，理由是...”；
- **关键作用**：将预训练的 “文本生成模型” 转化为 “对话模型”，初步具备遵循指令的能力。

### 2. RLHF：让模型 “说人话、守规矩”

- **三阶段流程**：
  1. **收集人类反馈**：让模型对同一问题生成多个回答，由人类标注员按 “有用性、安全性、相关性” 排序；
  2. **训练奖励模型（RM）**：用排序数据训练 RM，让模型能自动给回答打分（高分对应人类偏好的输出）；
  3. **强化学习优化（PPO）**：将 RM 作为 “奖励函数”，用 PPO 算法微调 ChatGPT，使其生成的回答在 RM 打分中更高（即更符合人类偏好）。
- **为何需要 RLHF**：SFT 只能让模型 “会回答”，但可能生成冗长、偏离主题或有害的内容，RLHF 通过人类反馈 “校准” 模型输出。

### 3. 能力边界与限制

- **优势场景**：
  - 开放域对话（闲聊、情感支持）；
  - 创意生成（写诗、编故事、写代码）；
  - 知识问答（常识、科普知识）；
  - 任务辅助（邮件撰写、翻译、总结）。
- **局限性**：
  - **时效性差**：训练数据截止到特定时间（如 GPT-3.5 截止到 2021 年），无法获取实时信息（如 2023 年后的新闻）；
  - **数学与逻辑短板**：复杂计算（如微积分）或多步推理（如复杂逻辑题）容易出错；
  - **对抗性攻击脆弱**：被诱导时可能生成有害内容（如 “如何制作危险物品”）；
  - **过度自信**：即使回答错误，也会用肯定语气表达（如虚构 “研究表明...”）。

### 4. 安全机制设计

- **输入过滤**：拦截明显有害的查询（如暴力、歧视相关）；
- **输出审查**：生成回答后通过额外模型检测是否包含敏感内容，若有则拒绝输出或修改；
- **持续迭代**：通过用户反馈（如 “举报不当回答”）更新安全规则，优化 RLHF 的数据与奖励模型。

