## chatGPT

chatGPT名字：“G”:generative;"P":Pre-trained;"transformer"

训练流程：预训练（无监督学习）--->监督学习--->强化学习（RL）

#### 自督导式学习（无监督学习）

 模型主要采用了两种无监督学习：

1. **掩码语言模型（Masked Language Model，MLM）**：模型会随机遮盖输入文本中的部分 token，例如将句子 “苹果是一种 [MASK] 水果” 输入模型，模型需要依据上下文 “苹果是一种” 以及语言知识，预测被遮盖的 “[MASK]” 处最有可能的词汇，比如 “红色的”“常见的” 等。

1. **下一句预测（Next Sentence Prediction，NSP）**：模型会接收两个句子，如 “A：小明去了超市。B：他买了一些水果。”，模型需要判断 B 句是否在逻辑上是 A 句的下一句。这有助于模型学习句子间的连贯性和逻辑关系，理解文本段落的组织架构。

#### 监督学习

用于在基座模型的基础上微调模型：

1. **数据收集与准备**：提供人工标注的数据

1. **模型微调过程**：利用标注好的数据对模型进行训练

#### 强化学习

引入人类偏好（奖励模型训练）：

1. **多样化输出与人工排序**：标注人员对模型的不同回复进行排序

1. **模型训练与偏好学习**：基于标注人员给出的回复排序结果，构建训练奖励模型（Reward Model，RM）的数据。采用 pair - wise learning to rank 方式，将排序后的回复两两组合，形成训练数据对。例如对于排序为回复 A＞回复 B 的情况，训练过程中鼓励奖励模型对＜提示，回复 A＞的打分高于＜提示，回复 B＞ 。RM 模型接收＜提示，回复＞输入对，输出一个反映回复质量高低的奖励分值。

近端策略优化（PPO）实现模型强化

1. **策略更新与奖励驱动**：利用近端策略优化（PPO，Proximal Policy Optimization）算法对 SFT 模型进行强化学习微调 。从用户提示中随机采样新提示，由 SFT 模型初始化 PPO 模型参数。PPO 模型根据输入提示生成回复，基于前一阶段训练好的 RM 模型为生成的＜提示，回复＞对打分，分数作为奖励反馈回 PPO 模型 。PPO 算法根据奖励信号调整模型策略，使得模型生成的回复更符合人类偏好，追求最大化奖励 。

1. **KL 散度约束与稳定性保障**：为防止模型过度优化奖励而偏离人类意图，PPO 算法引入从 SFT 模型输出衍生的监督约束，即计算新旧策略输出之间的 Kullback - Leibler（KL）散度 。KL 散度用于衡量两个概率分布的相似程度，在此处确保强化学习微调后的模型输出与 SFT 模型输出不至于偏差过大 。