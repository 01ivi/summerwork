# 模型压缩入门学习笔记

## 一、模型压缩概述

### 1.1 什么是模型压缩？

模型压缩是指在保证模型性能（精度、泛化能力等）损失可接受的前提下，通过一系列技术手段减小模型的大小、降低计算复杂度或减少内存占用的过程。 核心目标：**在 “性能” 与 “资源消耗” 之间找到平衡**，使大型深度学习模型（如 Transformer、ResNet 等）能够部署在资源受限的设备上（如手机、嵌入式设备、边缘计算节点）。

### 1.2 为什么需要模型压缩？

- **存储成本**：大型模型参数量可达数十亿甚至千亿（如 GPT-3 有 1750 亿参数），存储需求高达 GB 级，对存储设备压力大。
- **计算效率**：大模型推理时需要大量浮点运算（FLOPs），导致推理延迟高（如毫秒级延迟无法满足实时应用）。
- **部署限制**：移动端、物联网设备等硬件资源有限（算力、内存、电量均受限），无法运行未经压缩的大模型。
- **能耗问题**：大模型的高计算量会导致高能耗，不符合绿色 AI 的发展趋势。

### 1.3 模型压缩的核心指标

评估模型压缩效果的关键指标：



- **模型大小**：参数量（Params）或存储占用（如从 1GB 压缩到 100MB）。
- **计算量**：浮点运算次数（FLOPs）或每秒运算次数（TOPS）。
- **推理速度**：单样本推理时间（Latency，单位毫秒）。
- **性能损失**：压缩后模型在测试集上的精度下降（如准确率、mAP 等指标的变化）。



理想的压缩方法应实现：**模型大小 / 计算量显著降低，同时性能损失极小**。

### 1.4 模型压缩的主要方法分类

模型压缩技术可分为四大类（后续详细展开）：



- **知识蒸馏（Knowledge Distillation）**：用大模型（教师）指导小模型（学生）学习。
- **模型剪枝（Model Pruning）**：移除模型中 “不重要” 的参数或结构（如冗余权重、通道）。
- **参数量化（Parameter Quantization）**：用低比特数（如 8 位、4 位）表示参数，替代传统的 32 位浮点数。
- **架构搜索（Neural Architecture Search, NAS）**：直接设计轻量化模型结构（如 MobileNet、EfficientNet）。



此外，还有动态计算（如条件计算）、权重共享等辅助技术。

## 二、模型蒸馏入门（Knowledge Distillation）

### 2.1 核心思想

知识蒸馏通过 “教师 - 学生” 框架，让小模型（学生模型）学习大模型（教师模型）的 “知识”，从而在保持小模型规模的同时逼近大模型的性能。 这里的 “知识” 不仅包括教师模型的输出（如分类概率），还可包括中间层特征、注意力分布等。

### 2.2 基本原理

- **教师模型（Teacher Model）**：性能优异的预训练大模型（如 ResNet-152），作为知识的提供者。
- **学生模型（Student Model）**：待训练的小模型（如 ResNet-18），结构可与教师不同（更浅、更窄）。
- **蒸馏过程**：训练学生模型时，不仅让其拟合真实标签（任务损失），还让其模仿教师模型的输出（蒸馏损失），通过加权结合两种损失优化学生模型。

### 2.3 经典方法：Hinton 的蒸馏算法（2015）

- 软标签（Soft Labels）：教师模型的输出经过温度参数（Temperature, T）软化后，作为学生模型的学习目标。

  是模型的 logits（未归一化输出）。

  - 当 `T=1` 时，软标签退化为硬标签（one-hot）；
  - 当 `T>1` 时，概率分布更平缓，能传递更多类间关系（如 “猫” 与 “虎” 更相似）。

- **损失函数**：\(\mathcal{L} = \alpha \cdot \mathcal{L}_{\text{task}} + (1-\alpha) \cdot \mathcal{L}_{\text{distill}}\) 其中，`\mathcal{L}_{\text{task}}` 是学生模型与真实标签的交叉熵，`\mathcal{L}_{\text{distill}}` 是学生与教师软标签的 KL 散度，`\alpha` 是权重系数。

### 2.4 优缺点与适用场景

- **优点**：学生模型结构灵活，可针对部署场景定制；能传递教师模型的泛化能力。
- **缺点**：依赖高质量教师模型；需要额外训练过程，可能增加时间成本。
- **适用场景**：分类、NLP 等任务，尤其适合学生模型与教师模型结构差异较大的场景。

## 三、模型剪枝入门（Model Pruning）

### 3.1 核心思想

模型剪枝认为：训练好的神经网络存在大量冗余参数（如接近 0 的权重），这些参数对模型性能影响极小，可安全移除，从而减小模型规模并加速推理。 核心问题：**如何判断参数的 “重要性”**，以及如何移除后保持性能。

### 3.2 剪枝的基本流程

1. **训练原始模型**：得到一个性能较好的基线模型。
2. **评估参数重要性**：通过指标（如权重绝对值、梯度、对输出的影响）判断哪些参数可剪枝。
3. **剪枝冗余参数**：移除重要性低于阈值的参数，得到稀疏模型。
4. **微调（Fine-tuning）**：剪枝后模型性能可能下降，通过微调恢复精度（重新训练保留的参数）。
5. **迭代剪枝（可选）**：重复 “评估 - 剪枝 - 微调” 过程，进一步压缩模型。

### 3.3 剪枝的分类（按粒度）

- **非结构化剪枝（Unstructured Pruning）**： 剪枝单个权重参数（如将接近 0 的权重设为 0），可实现高稀疏度（如 90% 参数被剪）。 缺点：剪枝后模型是稀疏矩阵，需要专用硬件或软件支持才能加速（否则可能更慢）。
- **结构化剪枝（Structured Pruning）**： 剪枝具有结构性的组件（如整个卷积核、通道、层），剪枝后模型仍为密集矩阵，无需专用硬件即可加速。 例：移除卷积层中激活值较小的通道（Channel Pruning）。 优点：兼容性好，适合通用硬件（如 GPU、CPU）； 缺点：稀疏度通常低于非结构化剪枝。

### 3.4 经典方法举例

- **最优脑损伤（Optimal Brain Damage, OBD）**：通过二阶导数（Hessian 矩阵）衡量参数重要性，移除对损失函数影响小的参数。
- **通道剪枝（Channel Pruning）**：计算每个通道的重要性（如 L1 范数），剪去重要性低的通道，同时调整后续层的连接。

### 3.5 优缺点与适用场景

- **优点**：可显著减小模型参数量和计算量；无需改变模型结构（结构化剪枝）。
- **缺点**：需要精心设计重要性指标和微调策略；过度剪枝可能导致性能大幅下降。
- **适用场景**：卷积神经网络（CNN）、Transformer 等，尤其适合需要在通用硬件上部署的场景。

## 四、模型参数量化入门（Parameter Quantization）

### 4.1 核心思想

量化通过降低参数的数值精度（如从 32 位浮点数（FP32）转为 8 位整数（INT8）），减少模型的存储和计算成本。 原理：神经网络对噪声具有鲁棒性，参数不需要高精度表示即可保持性能。

### 4.2 量化的基本原理

- **映射关系**：将连续的浮点值（范围 `[x_min, x_max]`）映射到离散的整数（范围 `[q_min, q_max]`）。 例：FP32 值 `x` 量化为 INT8 值 `q` 的公式：\(q = \text{round}\left( \frac{x - x_{\text{offset}}}{s} \right)\) 其中，`s` 是缩放因子（`s = (x_max - x_min)/(q_max - q_min)`），`x_offset` 是偏移量。
- **反量化**：推理时，整数 `q` 需通过反量化公式还原为浮点值参与计算：\(x_{\text{approx}} = s \cdot q + x_{\text{offset}}\)

### 4.3 量化的分类

- **按量化位宽**：
  - 低精度量化：FP16（半精度）、BF16（脑浮点）、INT8（最常用）、INT4、甚至二进制（Binary）/ 三值（Ternary）量化。
  - 混合精度量化：对模型不同层使用不同位宽（如对敏感层用 FP16，对冗余层用 INT8）。
- **按量化阶段**：
  - **训练后量化（Post-Training Quantization, PTQ）**：直接对训练好的模型量化，无需重新训练，适合快速部署。但精度损失可能较大。
  - **量化感知训练（Quantization-Aware Training, QAT）**：在训练过程中模拟量化误差（如加入量化噪声），使模型适应低精度表示，精度损失更小，但需要额外训练成本。

### 4.4 优缺点与适用场景

- **优点**：实现简单，硬件兼容性好（多数芯片支持 INT8 计算）；可直接减少存储（如 INT8 比 FP32 节省 75% 存储空间）和计算量。
- **缺点**：过低位宽（如 INT4 以下）可能导致严重性能损失；部分任务（如生成式 AI）对精度敏感，量化难度大。
- **适用场景**：图像识别、语音识别等对精度要求不极端的任务；需要在嵌入式设备（如手机 CPU）上快速推理的场景。